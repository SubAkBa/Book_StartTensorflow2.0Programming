{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.4.1 - 단어 단위 생성",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzLgd7GNzYiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 예제 7.32 : 조선왕조실록 데이터 파일 다운로드\n",
        "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ys_pxumz81E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0465c428-eef2-4508-d4c4-97805c7e4fee"
      },
      "source": [
        "# 예제 7.33 : 데이터 로드 및 확인\n",
        "# 데이터를 메모리에 불러옵니다. 인코딩 형식으로 utf-8을 지정해야 합니다.\n",
        "train_text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
        "\n",
        "# 텍스트가 총 몇 자인지 확인합니다.\n",
        "print('Length of text : {} characters'.format(len(train_text)))\n",
        "print()\n",
        "\n",
        "# 처음 100자를 확인해봅니다.\n",
        "print(train_text[: 100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text : 26265493 characters\n",
            "\n",
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
            "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM_Fnvy30ztk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0afd2f41-8b2d-4be0-d58f-a642285a4014"
      },
      "source": [
        "# 에제 7.34 : 훈련 데이터 입력 정제\n",
        "import re\n",
        "# From https://github.com/yoonkim/CNN_sentence/blob/master/process.data.py\n",
        "def clean_str(string):\n",
        "  string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "  string = re.sub(r\",\", \" , \", string) \n",
        "  string = re.sub(r\"!\", \" ! \", string) \n",
        "  string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "  string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "  string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "  string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
        "  string = re.sub(r\"\\'\", \"\", string)\n",
        "\n",
        "  return string\n",
        "\n",
        "train_text = train_text.split('\\n')\n",
        "train_text = [clean_str(sentence) for sentence in train_text]\n",
        "train_text_X = []\n",
        "\n",
        "for sentence in train_text:\n",
        "  train_text_X.extend(sentence.split(' '))\n",
        "  train_text_X.append('\\n')\n",
        "\n",
        "train_text_X = [word for word in train_text_X if word != '']\n",
        "\n",
        "print(train_text_X[: 20])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrPStSrd18GF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "63607520-f1a6-49a4-cdd3-0e47b571b7e2"
      },
      "source": [
        "# 예제 7.35 : 단어 토큰화\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 단어의 set을 만듭니다.\n",
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "print('{} unique words'.format(len(vocab)))\n",
        "\n",
        "# vocab list를 숫자로 매핑하고, 반대도 실행합니다.\n",
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
        "\n",
        "# word2idx 의 일부를 알아보기 쉽게 출력해 봅니다.\n",
        "print('{')\n",
        "for word, _ in zip(word2idx, range(10)):\n",
        "  print(' {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
        "print(' ...\\n}')\n",
        "\n",
        "print('index of UNK: {}'.format(word2idx['UNK']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "332515 unique words\n",
            "{\n",
            " '\\n':   0,\n",
            " '!' :   1,\n",
            " ',' :   2,\n",
            " '000명으로':   3,\n",
            " '001':   4,\n",
            " '002':   5,\n",
            " '003':   6,\n",
            " '004':   7,\n",
            " '005':   8,\n",
            " '006':   9,\n",
            " ...\n",
            "}\n",
            "index of UNK: 332514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXi_IUAk2w3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c92d6ebe-c0f4-48f6-9669-8ee81a648fd6"
      },
      "source": [
        "# 예제 7.36 : 토큰 데이터 확인\n",
        "print(train_text_X[: 20])\n",
        "print(text_as_int[: 20])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
            "[299181 229520 161349  17366 110944 230178 250960 154993 225348  28960\n",
            " 190192 256005      0 299181  25557 273428  36069 163902 180371  84330]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKwsUl53A9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dcdb01f8-f360-4ddd-d811-5ab6f33e8487"
      },
      "source": [
        "# 예제 7.37 : 기본 데이터세트 만들기\n",
        "seq_length = 25\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sentence_dataset = sentence_dataset.batch(seq_length + 1, drop_remainder = True)\n",
        "\n",
        "for item in sentence_dataset.take(1):\n",
        "  print(idx2word[item.numpy()])\n",
        "  print(item.numpy())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '\\\\(' '\\\\)' '의' '성은' '이씨' '\\\\(']\n",
            "[299181 229520 161349  17366 110944 230178 250960 154993 225348  28960\n",
            " 190192 256005      0 299181  25557 273428  36069 163902 180371  84330\n",
            "  17278  17279 224068 164455 230134  17278]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJV12Pgq3icj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "18d63a4e-7753-4955-b251-4adf112616c8"
      },
      "source": [
        "# 예제 7.38 : 학습 데이터세트 만들기\n",
        "def split_input_target(chunk):\n",
        "  return [chunk[: -1], chunk[-1]]\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "for x, y in train_dataset.take(1):\n",
        "  print(idx2word[x.numpy()])\n",
        "  print(x.numpy())\n",
        "  print(idx2word[y.numpy()])\n",
        "  print(y.numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '\\\\(' '\\\\)' '의' '성은' '이씨']\n",
            "[299181 229520 161349  17366 110944 230178 250960 154993 225348  28960\n",
            " 190192 256005      0 299181  25557 273428  36069 163902 180371  84330\n",
            "  17278  17279 224068 164455 230134]\n",
            "\\(\n",
            "17278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LABJ3rXHrU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예제 7.39 : 데이터세트 shuffle, batch 설정\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPCY1cbXIGek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "85b7eb3b-d965-4f2a-e014-af295c6c1e79"
      },
      "source": [
        "# 예제 7.40 : 단어 단위 생성 모델 정의\n",
        "total_words = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(total_words, 100, input_length = seq_length),\n",
        "                             tf.keras.layers.LSTM(units = 100, return_sequences = True),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.LSTM(units = 100),\n",
        "                             tf.keras.layers.Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 25, 100)           33251500  \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 25, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 332515)            33584015  \n",
            "=================================================================\n",
            "Total params: 66,996,315\n",
            "Trainable params: 66,996,315\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byFMQ74sIm4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "871ac949-e36b-4ca5-f59d-6084d7b982c5"
      },
      "source": [
        "# 예제 7.41 : 단어 단위 생성 모델 학습\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def testmodel(epoch, logs):\n",
        "  if epoch % 5 != 0 and epoch != 49:\n",
        "    return\n",
        "  test_sentence = train_text[0]\n",
        "\n",
        "  next_words = 100\n",
        "  for _ in range(next_words):\n",
        "    test_text_X = test_sentence.split(' ')[-seq_length :]\n",
        "    test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen = seq_length, padding = 'pre', value = word2idx['UNK'])\n",
        "\n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += ' '+ idx2word[output_idx[0]]\n",
        "\n",
        "  print()\n",
        "  print(test_sentence)\n",
        "  print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end = testmodel)\n",
        "\n",
        "history = model.fit(train_dataset.repeat(), epochs = 50, steps_per_epoch = steps_per_epoch, callbacks = [testmodelcb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2184/2719 [=======================>......] - ETA: 14:40 - loss: 7.7264 - accuracy: 0.1865"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRJKS2WfKMDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예제 7.42 : 임의의 문장을 사용한 생성 결과 확인\n",
        "from tensorflow.keras.preprocessing import pad_sequences\n",
        "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
        "\n",
        "next_words = 100\n",
        "for _ in range(next_words):\n",
        "  test_text_X = test_sentence.split(' ')[-seq_length :]\n",
        "  test_text_X = np.array([word2idx[c] if c in word2idx else total_words + 1 for c in test_text_X])\n",
        "  test_text_X = pad_sequences([test_text_X], maxlen = seq_length, padding = 'pre', value = word2idx['UNK'])\n",
        "  \n",
        "  output_idx = model.predict_classes(test_text_X)\n",
        "  test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "\n",
        "print(test_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}